{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Model-Based RL and Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this Notebook the Cart-Pole task is going to be solved with model-based RL and learned dynamics. This means that a network is used to learn the model dynamics, and one serves as the Actor. The interaction with the environment is needed for both networks to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL and networks params\n",
    "\n",
    "H = 8\n",
    "lr = 1e-2\n",
    "gamma = .99\n",
    "decay_rate = .99\n",
    "resume = False\n",
    "\n",
    "# Batch Sizes\n",
    "model_bs = 3\n",
    "real_bs = 3\n",
    "\n",
    "D = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "####    \n",
    "\n",
    "The Actor network is a simple single hidden layer NN with H neurons. Takes as input the state of the environment and outputs the action.\n",
    "The objective is to maximize the log Likelihood (since the action choice is a 2-classes classification problem), training is done with Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Actor Network\n",
    "# Simple 1-Hidden layer network (sigmoid output layer)\n",
    "observations = tf.placeholder(shape=[None, 4], dtype=tf.float32, name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H], initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "# \n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"input_y\")\n",
    "advantages = tf.placeholder(dtype=tf.float32, name=\"reward_signal\")\n",
    "optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages)\n",
    "newGrads = tf.gradients(loss, tvars)\n",
    "updateGrads = optim.apply_gradients(zip(batchGrad, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####         \n",
    "\n",
    "\n",
    "The Model Network is a bit more complex: the input is the vector $(state, action)$, the outputs are: \n",
    "* Predicted State (4-dimensional)\n",
    "* Predicted Reward (1-dimensional)\n",
    "* Predicted End (1-dimensional)\n",
    "\n",
    "The network has 2 hidden layers shared among all the outputs, while the last layer has different weights for each output. The objective of the network is the mean of the losses of the single outputs (squared error for state and reward prediction - regression - and log likelihood for the predicted end - two classes classification). Training is done with Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Network\n",
    "mH = 256\n",
    "\n",
    "input_data = tf.placeholder(shape=[None, 5], dtype=tf.float32)\n",
    "  \n",
    "# Shared Part of the network\n",
    "previous_state = tf.placeholder(dtype=tf.float32, shape=[None, 5], name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]), name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state, W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]), name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M, W2M) + B2M)\n",
    "\n",
    "# Different final layers for each output\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4], initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]), name=\"b0\")\n",
    "bR = tf.Variable(tf.zeros([1]), name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]), name=\"bD\")\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M, wO, name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M, wR, name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M, wD, name=\"predicted_observation\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(dtype=tf.float32, shape=[None, 4], name=\"true_observation\")\n",
    "true_reward = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"true_reward\")\n",
    "true_done = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"true_done\")\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation, predicted_reward, predicted_done], 1)\n",
    "\n",
    "# Losses and optimizer\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelOptim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "updateModel = modelOptim.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "\n",
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = r[t] + gamma * running_add\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0], np.array(action)]), [1, 5])\n",
    "    myPredict = sess.run([predicted_state], feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:, 4]\n",
    "    observation = myPredict[0][:, :4]\n",
    "    observation[:, 0] = np.clip(observation[:, 0], -2.4, 2.4)\n",
    "    observation[:, 2] = np.clip(observation[:, 2], -2.4, 2.4)\n",
    "    doneP = np.clip(myPredict[0][:, 5], 0, 1)\n",
    "    if doneP > 0.1 or len(xs) >= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#     \n",
    "The networks are trained using the whole episode (on-policy RL - similar to MC) in this way:\n",
    "1. The **Model DNN is trained** taking actions on the **real environment**, using real observations and rewards\n",
    "2. The **Actor DNN is trained** taking actions on the **Model**, using the predicted observations and rewards\n",
    "\n",
    "These two phases are alternated, after the first 100 episodes of Model DNN training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4.000000. Reward 22.333333. action: 1.000000. mean reward 22.333333.\n",
      "World Perf: Episode 7.000000. Reward 17.333333. action: 1.000000. mean reward 22.283333.\n",
      "World Perf: Episode 10.000000. Reward 15.666667. action: 1.000000. mean reward 22.217167.\n",
      "World Perf: Episode 13.000000. Reward 17.666667. action: 1.000000. mean reward 22.171662.\n",
      "World Perf: Episode 16.000000. Reward 18.666667. action: 0.000000. mean reward 22.136612.\n",
      "World Perf: Episode 19.000000. Reward 20.333333. action: 0.000000. mean reward 22.118579.\n",
      "World Perf: Episode 22.000000. Reward 26.333333. action: 1.000000. mean reward 22.160726.\n",
      "World Perf: Episode 25.000000. Reward 16.333333. action: 1.000000. mean reward 22.102453.\n",
      "World Perf: Episode 28.000000. Reward 19.000000. action: 1.000000. mean reward 22.071428.\n",
      "World Perf: Episode 31.000000. Reward 22.333333. action: 1.000000. mean reward 22.074047.\n",
      "World Perf: Episode 34.000000. Reward 15.333333. action: 1.000000. mean reward 22.006640.\n",
      "World Perf: Episode 37.000000. Reward 12.000000. action: 0.000000. mean reward 21.906574.\n",
      "World Perf: Episode 40.000000. Reward 15.000000. action: 1.000000. mean reward 21.837508.\n",
      "World Perf: Episode 43.000000. Reward 20.000000. action: 1.000000. mean reward 21.819133.\n",
      "World Perf: Episode 46.000000. Reward 22.333333. action: 1.000000. mean reward 21.824275.\n",
      "World Perf: Episode 49.000000. Reward 15.666667. action: 1.000000. mean reward 21.762699.\n",
      "World Perf: Episode 52.000000. Reward 21.000000. action: 0.000000. mean reward 21.755072.\n",
      "World Perf: Episode 55.000000. Reward 12.000000. action: 1.000000. mean reward 21.657521.\n",
      "World Perf: Episode 58.000000. Reward 13.666667. action: 1.000000. mean reward 21.577612.\n",
      "World Perf: Episode 61.000000. Reward 15.666667. action: 1.000000. mean reward 21.518503.\n",
      "World Perf: Episode 64.000000. Reward 18.000000. action: 1.000000. mean reward 21.483318.\n",
      "World Perf: Episode 67.000000. Reward 17.666667. action: 1.000000. mean reward 21.445151.\n",
      "World Perf: Episode 70.000000. Reward 12.000000. action: 0.000000. mean reward 21.350700.\n",
      "World Perf: Episode 73.000000. Reward 19.333333. action: 1.000000. mean reward 21.330526.\n",
      "World Perf: Episode 76.000000. Reward 20.333333. action: 0.000000. mean reward 21.320554.\n",
      "World Perf: Episode 79.000000. Reward 17.333333. action: 1.000000. mean reward 21.280682.\n",
      "World Perf: Episode 82.000000. Reward 23.000000. action: 0.000000. mean reward 21.297875.\n",
      "World Perf: Episode 85.000000. Reward 16.000000. action: 1.000000. mean reward 21.244897.\n",
      "World Perf: Episode 88.000000. Reward 24.333333. action: 0.000000. mean reward 21.275781.\n",
      "World Perf: Episode 91.000000. Reward 13.333333. action: 1.000000. mean reward 21.196356.\n",
      "World Perf: Episode 94.000000. Reward 12.333333. action: 1.000000. mean reward 21.107726.\n",
      "World Perf: Episode 97.000000. Reward 15.333333. action: 1.000000. mean reward 21.049982.\n",
      "World Perf: Episode 100.000000. Reward 19.666667. action: 0.000000. mean reward 21.036149.\n",
      "World Perf: Episode 103.000000. Reward 17.333333. action: 1.000000. mean reward 20.999121.\n",
      "World Perf: Episode 106.000000. Reward 20.000000. action: 1.000000. mean reward 20.904161.\n",
      "World Perf: Episode 109.000000. Reward 17.333333. action: 0.000000. mean reward 20.767525.\n",
      "World Perf: Episode 112.000000. Reward 14.666667. action: 1.000000. mean reward 20.569025.\n",
      "World Perf: Episode 115.000000. Reward 15.000000. action: 1.000000. mean reward 20.373508.\n",
      "World Perf: Episode 118.000000. Reward 18.333333. action: 1.000000. mean reward 20.409363.\n",
      "World Perf: Episode 121.000000. Reward 28.333333. action: 1.000000. mean reward 20.515684.\n",
      "World Perf: Episode 124.000000. Reward 15.666667. action: 1.000000. mean reward 20.598217.\n",
      "World Perf: Episode 127.000000. Reward 17.000000. action: 1.000000. mean reward 20.496382.\n",
      "World Perf: Episode 130.000000. Reward 14.666667. action: 0.000000. mean reward 20.461811.\n",
      "World Perf: Episode 133.000000. Reward 18.666667. action: 1.000000. mean reward 20.311190.\n",
      "World Perf: Episode 136.000000. Reward 17.000000. action: 1.000000. mean reward 20.220930.\n",
      "World Perf: Episode 139.000000. Reward 22.333333. action: 1.000000. mean reward 20.224764.\n",
      "World Perf: Episode 142.000000. Reward 19.333333. action: 0.000000. mean reward 20.104710.\n",
      "World Perf: Episode 145.000000. Reward 21.666667. action: 0.000000. mean reward 20.244301.\n",
      "World Perf: Episode 148.000000. Reward 28.333333. action: 1.000000. mean reward 20.410196.\n",
      "World Perf: Episode 151.000000. Reward 21.000000. action: 0.000000. mean reward 20.328594.\n",
      "World Perf: Episode 154.000000. Reward 23.333333. action: 1.000000. mean reward 20.248869.\n",
      "World Perf: Episode 157.000000. Reward 15.333333. action: 1.000000. mean reward 20.056932.\n",
      "World Perf: Episode 160.000000. Reward 19.666667. action: 1.000000. mean reward 19.941149.\n",
      "World Perf: Episode 163.000000. Reward 23.333333. action: 1.000000. mean reward 19.852571.\n",
      "World Perf: Episode 166.000000. Reward 32.333333. action: 0.000000. mean reward 19.933847.\n",
      "World Perf: Episode 169.000000. Reward 18.666667. action: 1.000000. mean reward 20.201082.\n",
      "World Perf: Episode 172.000000. Reward 24.333333. action: 1.000000. mean reward 20.123652.\n",
      "World Perf: Episode 175.000000. Reward 15.333333. action: 1.000000. mean reward 19.979723.\n",
      "World Perf: Episode 178.000000. Reward 41.000000. action: 0.000000. mean reward 20.169102.\n",
      "World Perf: Episode 181.000000. Reward 26.000000. action: 1.000000. mean reward 20.313587.\n",
      "World Perf: Episode 184.000000. Reward 27.333333. action: 1.000000. mean reward 20.358858.\n",
      "World Perf: Episode 187.000000. Reward 29.333333. action: 1.000000. mean reward 23.038544.\n",
      "World Perf: Episode 190.000000. Reward 29.000000. action: 1.000000. mean reward 22.919861.\n",
      "World Perf: Episode 193.000000. Reward 35.000000. action: 1.000000. mean reward 22.895216.\n",
      "World Perf: Episode 196.000000. Reward 35.000000. action: 1.000000. mean reward 22.882757.\n",
      "World Perf: Episode 199.000000. Reward 21.666667. action: 0.000000. mean reward 22.800707.\n",
      "World Perf: Episode 202.000000. Reward 33.666667. action: 1.000000. mean reward 22.851913.\n",
      "World Perf: Episode 205.000000. Reward 24.000000. action: 0.000000. mean reward 22.791323.\n",
      "World Perf: Episode 208.000000. Reward 21.000000. action: 0.000000. mean reward 22.903357.\n",
      "World Perf: Episode 211.000000. Reward 31.333333. action: 1.000000. mean reward 22.872192.\n",
      "World Perf: Episode 214.000000. Reward 14.666667. action: 1.000000. mean reward 22.791800.\n",
      "World Perf: Episode 217.000000. Reward 25.000000. action: 0.000000. mean reward 22.757116.\n",
      "World Perf: Episode 220.000000. Reward 26.000000. action: 0.000000. mean reward 23.667055.\n",
      "World Perf: Episode 223.000000. Reward 38.333333. action: 0.000000. mean reward 23.882357.\n",
      "World Perf: Episode 226.000000. Reward 31.000000. action: 1.000000. mean reward 23.954248.\n",
      "World Perf: Episode 229.000000. Reward 26.666667. action: 1.000000. mean reward 23.992502.\n",
      "World Perf: Episode 232.000000. Reward 40.000000. action: 0.000000. mean reward 24.303579.\n",
      "World Perf: Episode 235.000000. Reward 14.333333. action: 1.000000. mean reward 24.152750.\n",
      "World Perf: Episode 238.000000. Reward 24.000000. action: 1.000000. mean reward 24.079363.\n",
      "World Perf: Episode 241.000000. Reward 39.666667. action: 0.000000. mean reward 24.462469.\n",
      "World Perf: Episode 244.000000. Reward 23.666667. action: 0.000000. mean reward 24.651487.\n",
      "World Perf: Episode 247.000000. Reward 34.666667. action: 0.000000. mean reward 24.691742.\n",
      "World Perf: Episode 250.000000. Reward 46.333333. action: 1.000000. mean reward 25.185587.\n",
      "World Perf: Episode 253.000000. Reward 25.666667. action: 0.000000. mean reward 25.638466.\n",
      "World Perf: Episode 256.000000. Reward 61.333333. action: 1.000000. mean reward 26.340645.\n",
      "World Perf: Episode 259.000000. Reward 55.333333. action: 0.000000. mean reward 26.480600.\n",
      "World Perf: Episode 262.000000. Reward 23.000000. action: 0.000000. mean reward 26.332151.\n",
      "World Perf: Episode 265.000000. Reward 38.000000. action: 0.000000. mean reward 26.521788.\n",
      "World Perf: Episode 268.000000. Reward 32.666667. action: 0.000000. mean reward 26.437241.\n",
      "World Perf: Episode 271.000000. Reward 33.000000. action: 0.000000. mean reward 26.484430.\n",
      "World Perf: Episode 274.000000. Reward 22.000000. action: 1.000000. mean reward 26.239311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 277.000000. Reward 46.666667. action: 1.000000. mean reward 26.325121.\n",
      "World Perf: Episode 280.000000. Reward 52.000000. action: 1.000000. mean reward 26.547165.\n",
      "World Perf: Episode 283.000000. Reward 42.333333. action: 0.000000. mean reward 26.679253.\n",
      "World Perf: Episode 286.000000. Reward 53.666667. action: 0.000000. mean reward 27.034597.\n",
      "World Perf: Episode 289.000000. Reward 25.333333. action: 1.000000. mean reward 27.496367.\n",
      "World Perf: Episode 292.000000. Reward 49.333333. action: 1.000000. mean reward 27.518257.\n",
      "World Perf: Episode 295.000000. Reward 54.666667. action: 1.000000. mean reward 27.574442.\n",
      "World Perf: Episode 298.000000. Reward 27.000000. action: 1.000000. mean reward 27.402647.\n",
      "World Perf: Episode 301.000000. Reward 29.333333. action: 0.000000. mean reward 27.236933.\n",
      "World Perf: Episode 304.000000. Reward 30.333333. action: 0.000000. mean reward 27.060095.\n",
      "World Perf: Episode 307.000000. Reward 57.666667. action: 0.000000. mean reward 27.865881.\n",
      "World Perf: Episode 310.000000. Reward 54.666667. action: 0.000000. mean reward 28.016891.\n",
      "World Perf: Episode 313.000000. Reward 30.666667. action: 0.000000. mean reward 28.072174.\n",
      "World Perf: Episode 316.000000. Reward 46.333333. action: 1.000000. mean reward 28.901657.\n",
      "World Perf: Episode 319.000000. Reward 35.000000. action: 1.000000. mean reward 29.756630.\n",
      "World Perf: Episode 322.000000. Reward 31.000000. action: 1.000000. mean reward 30.356600.\n",
      "World Perf: Episode 325.000000. Reward 60.000000. action: 0.000000. mean reward 30.550550.\n",
      "World Perf: Episode 328.000000. Reward 37.000000. action: 1.000000. mean reward 30.659311.\n",
      "World Perf: Episode 331.000000. Reward 19.333333. action: 0.000000. mean reward 30.460516.\n",
      "World Perf: Episode 334.000000. Reward 62.333333. action: 0.000000. mean reward 30.731331.\n",
      "World Perf: Episode 337.000000. Reward 44.333333. action: 0.000000. mean reward 30.908043.\n",
      "World Perf: Episode 340.000000. Reward 54.666667. action: 0.000000. mean reward 31.112968.\n",
      "World Perf: Episode 343.000000. Reward 35.333333. action: 0.000000. mean reward 30.932701.\n",
      "World Perf: Episode 346.000000. Reward 46.666667. action: 1.000000. mean reward 31.432440.\n",
      "World Perf: Episode 349.000000. Reward 50.333333. action: 0.000000. mean reward 31.796602.\n",
      "World Perf: Episode 352.000000. Reward 36.333333. action: 1.000000. mean reward 33.611710.\n",
      "World Perf: Episode 355.000000. Reward 50.333333. action: 1.000000. mean reward 33.602688.\n",
      "World Perf: Episode 358.000000. Reward 35.333333. action: 0.000000. mean reward 33.757900.\n",
      "World Perf: Episode 361.000000. Reward 43.000000. action: 1.000000. mean reward 33.837246.\n",
      "World Perf: Episode 364.000000. Reward 70.333333. action: 0.000000. mean reward 33.938175.\n",
      "World Perf: Episode 367.000000. Reward 32.666667. action: 0.000000. mean reward 33.807186.\n",
      "World Perf: Episode 370.000000. Reward 120.333333. action: 0.000000. mean reward 35.035114.\n",
      "World Perf: Episode 373.000000. Reward 42.333333. action: 1.000000. mean reward 35.012730.\n",
      "World Perf: Episode 376.000000. Reward 45.333333. action: 0.000000. mean reward 37.794640.\n",
      "World Perf: Episode 379.000000. Reward 41.000000. action: 1.000000. mean reward 38.069210.\n",
      "World Perf: Episode 382.000000. Reward 58.666667. action: 1.000000. mean reward 38.057652.\n",
      "World Perf: Episode 385.000000. Reward 28.333333. action: 1.000000. mean reward 37.701626.\n",
      "World Perf: Episode 388.000000. Reward 49.333333. action: 1.000000. mean reward 37.632973.\n",
      "World Perf: Episode 391.000000. Reward 54.333333. action: 0.000000. mean reward 37.490097.\n",
      "World Perf: Episode 394.000000. Reward 64.000000. action: 0.000000. mean reward 37.610046.\n",
      "World Perf: Episode 397.000000. Reward 70.666667. action: 0.000000. mean reward 37.626690.\n",
      "World Perf: Episode 400.000000. Reward 66.666667. action: 1.000000. mean reward 37.854740.\n",
      "World Perf: Episode 403.000000. Reward 47.000000. action: 0.000000. mean reward 39.544659.\n",
      "World Perf: Episode 406.000000. Reward 91.000000. action: 0.000000. mean reward 39.712963.\n",
      "World Perf: Episode 409.000000. Reward 68.333333. action: 0.000000. mean reward 42.571587.\n",
      "World Perf: Episode 412.000000. Reward 42.333333. action: 0.000000. mean reward 42.331570.\n",
      "World Perf: Episode 415.000000. Reward 57.333333. action: 0.000000. mean reward 42.840649.\n",
      "World Perf: Episode 418.000000. Reward 104.666667. action: 0.000000. mean reward 43.202129.\n",
      "World Perf: Episode 421.000000. Reward 80.666667. action: 1.000000. mean reward 43.346889.\n",
      "World Perf: Episode 424.000000. Reward 63.000000. action: 0.000000. mean reward 43.329075.\n",
      "World Perf: Episode 427.000000. Reward 28.666667. action: 0.000000. mean reward 42.903095.\n",
      "World Perf: Episode 430.000000. Reward 61.333333. action: 0.000000. mean reward 42.975422.\n",
      "World Perf: Episode 433.000000. Reward 39.333333. action: 0.000000. mean reward 42.933762.\n",
      "World Perf: Episode 436.000000. Reward 36.666667. action: 1.000000. mean reward 42.531483.\n",
      "World Perf: Episode 439.000000. Reward 27.333333. action: 0.000000. mean reward 42.088985.\n",
      "World Perf: Episode 442.000000. Reward 62.000000. action: 0.000000. mean reward 42.038803.\n",
      "World Perf: Episode 445.000000. Reward 63.666667. action: 0.000000. mean reward 42.020370.\n",
      "World Perf: Episode 448.000000. Reward 41.333333. action: 0.000000. mean reward 41.950020.\n",
      "World Perf: Episode 451.000000. Reward 44.666667. action: 1.000000. mean reward 44.504230.\n",
      "World Perf: Episode 454.000000. Reward 37.666667. action: 0.000000. mean reward 44.529873.\n",
      "World Perf: Episode 457.000000. Reward 76.000000. action: 1.000000. mean reward 44.449207.\n",
      "World Perf: Episode 460.000000. Reward 73.333333. action: 1.000000. mean reward 46.269100.\n",
      "World Perf: Episode 463.000000. Reward 72.000000. action: 0.000000. mean reward 46.786530.\n",
      "World Perf: Episode 466.000000. Reward 29.000000. action: 1.000000. mean reward 46.315323.\n",
      "World Perf: Episode 469.000000. Reward 51.666667. action: 0.000000. mean reward 46.087940.\n",
      "World Perf: Episode 472.000000. Reward 22.333333. action: 0.000000. mean reward 45.977573.\n",
      "World Perf: Episode 475.000000. Reward 43.333333. action: 0.000000. mean reward 45.572598.\n",
      "World Perf: Episode 478.000000. Reward 35.666667. action: 0.000000. mean reward 45.287189.\n",
      "World Perf: Episode 481.000000. Reward 40.000000. action: 0.000000. mean reward 46.629681.\n",
      "World Perf: Episode 484.000000. Reward 75.666667. action: 0.000000. mean reward 46.760586.\n",
      "World Perf: Episode 487.000000. Reward 67.000000. action: 0.000000. mean reward 48.937489.\n",
      "World Perf: Episode 490.000000. Reward 64.666667. action: 1.000000. mean reward 49.998596.\n",
      "World Perf: Episode 493.000000. Reward 53.666667. action: 0.000000. mean reward 50.195202.\n",
      "World Perf: Episode 496.000000. Reward 46.666667. action: 1.000000. mean reward 49.863907.\n",
      "World Perf: Episode 499.000000. Reward 96.000000. action: 1.000000. mean reward 49.994839.\n",
      "World Perf: Episode 502.000000. Reward 74.000000. action: 1.000000. mean reward 49.833710.\n",
      "World Perf: Episode 505.000000. Reward 105.333333. action: 1.000000. mean reward 50.068333.\n",
      "World Perf: Episode 508.000000. Reward 96.333333. action: 1.000000. mean reward 50.934246.\n",
      "World Perf: Episode 511.000000. Reward 67.333333. action: 0.000000. mean reward 54.718067.\n",
      "World Perf: Episode 514.000000. Reward 21.000000. action: 0.000000. mean reward 54.126492.\n",
      "World Perf: Episode 517.000000. Reward 56.666667. action: 0.000000. mean reward 53.679531.\n",
      "World Perf: Episode 520.000000. Reward 21.000000. action: 0.000000. mean reward 52.942295.\n",
      "World Perf: Episode 523.000000. Reward 46.000000. action: 1.000000. mean reward 52.433121.\n",
      "World Perf: Episode 526.000000. Reward 65.333333. action: 1.000000. mean reward 52.140369.\n",
      "World Perf: Episode 529.000000. Reward 48.666667. action: 1.000000. mean reward 51.651417.\n",
      "World Perf: Episode 532.000000. Reward 68.000000. action: 1.000000. mean reward 51.423115.\n",
      "World Perf: Episode 535.000000. Reward 58.666667. action: 1.000000. mean reward 53.701847.\n",
      "World Perf: Episode 538.000000. Reward 53.000000. action: 1.000000. mean reward 53.582592.\n",
      "World Perf: Episode 541.000000. Reward 105.666667. action: 0.000000. mean reward 56.710308.\n",
      "World Perf: Episode 544.000000. Reward 76.666667. action: 0.000000. mean reward 59.530079.\n",
      "World Perf: Episode 547.000000. Reward 69.333333. action: 0.000000. mean reward 59.122543.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 550.000000. Reward 42.000000. action: 1.000000. mean reward 61.328968.\n",
      "World Perf: Episode 553.000000. Reward 47.000000. action: 0.000000. mean reward 60.714920.\n",
      "World Perf: Episode 556.000000. Reward 57.666667. action: 1.000000. mean reward 60.166168.\n",
      "World Perf: Episode 559.000000. Reward 37.666667. action: 1.000000. mean reward 59.692135.\n",
      "World Perf: Episode 562.000000. Reward 38.333333. action: 1.000000. mean reward 59.007572.\n",
      "World Perf: Episode 565.000000. Reward 70.333333. action: 0.000000. mean reward 59.112961.\n",
      "World Perf: Episode 568.000000. Reward 87.333333. action: 0.000000. mean reward 59.443542.\n",
      "World Perf: Episode 571.000000. Reward 50.333333. action: 1.000000. mean reward 59.878445.\n",
      "World Perf: Episode 574.000000. Reward 39.000000. action: 1.000000. mean reward 61.954762.\n",
      "World Perf: Episode 577.000000. Reward 53.000000. action: 1.000000. mean reward 61.512684.\n",
      "World Perf: Episode 580.000000. Reward 29.666667. action: 1.000000. mean reward 62.910248.\n",
      "World Perf: Episode 583.000000. Reward 85.333333. action: 1.000000. mean reward 62.843761.\n",
      "World Perf: Episode 586.000000. Reward 42.333333. action: 0.000000. mean reward 62.125187.\n",
      "World Perf: Episode 589.000000. Reward 36.000000. action: 0.000000. mean reward 61.439777.\n",
      "World Perf: Episode 592.000000. Reward 90.333333. action: 0.000000. mean reward 61.348240.\n",
      "World Perf: Episode 595.000000. Reward 43.666667. action: 0.000000. mean reward 60.744068.\n",
      "World Perf: Episode 598.000000. Reward 70.000000. action: 0.000000. mean reward 60.364254.\n",
      "World Perf: Episode 601.000000. Reward 61.666667. action: 0.000000. mean reward 60.640568.\n",
      "World Perf: Episode 604.000000. Reward 52.333333. action: 0.000000. mean reward 61.083691.\n",
      "World Perf: Episode 607.000000. Reward 52.333333. action: 1.000000. mean reward 60.515209.\n",
      "World Perf: Episode 610.000000. Reward 80.333333. action: 0.000000. mean reward 60.714870.\n",
      "World Perf: Episode 613.000000. Reward 58.000000. action: 1.000000. mean reward 60.169712.\n",
      "World Perf: Episode 616.000000. Reward 96.666667. action: 1.000000. mean reward 60.000443.\n",
      "World Perf: Episode 619.000000. Reward 70.666667. action: 0.000000. mean reward 59.567966.\n",
      "World Perf: Episode 622.000000. Reward 88.666667. action: 0.000000. mean reward 59.298100.\n",
      "World Perf: Episode 625.000000. Reward 56.666667. action: 0.000000. mean reward 58.746799.\n",
      "World Perf: Episode 628.000000. Reward 87.333333. action: 1.000000. mean reward 58.558811.\n",
      "World Perf: Episode 631.000000. Reward 72.333333. action: 0.000000. mean reward 58.172943.\n",
      "World Perf: Episode 634.000000. Reward 67.333333. action: 1.000000. mean reward 58.747059.\n",
      "World Perf: Episode 637.000000. Reward 76.333333. action: 0.000000. mean reward 58.653439.\n",
      "World Perf: Episode 640.000000. Reward 79.000000. action: 1.000000. mean reward 58.939529.\n",
      "World Perf: Episode 643.000000. Reward 91.666667. action: 0.000000. mean reward 59.095478.\n",
      "World Perf: Episode 646.000000. Reward 82.666667. action: 1.000000. mean reward 59.090626.\n",
      "World Perf: Episode 649.000000. Reward 94.000000. action: 1.000000. mean reward 58.921207.\n",
      "World Perf: Episode 652.000000. Reward 75.333333. action: 0.000000. mean reward 58.772778.\n",
      "World Perf: Episode 655.000000. Reward 78.000000. action: 1.000000. mean reward 58.660812.\n",
      "World Perf: Episode 658.000000. Reward 96.333333. action: 1.000000. mean reward 61.530552.\n",
      "World Perf: Episode 661.000000. Reward 64.333333. action: 1.000000. mean reward 61.092052.\n",
      "World Perf: Episode 664.000000. Reward 67.333333. action: 0.000000. mean reward 60.675983.\n",
      "World Perf: Episode 667.000000. Reward 117.666667. action: 0.000000. mean reward 61.048756.\n",
      "World Perf: Episode 670.000000. Reward 39.333333. action: 1.000000. mean reward 60.283604.\n",
      "World Perf: Episode 673.000000. Reward 57.333333. action: 0.000000. mean reward 59.724514.\n",
      "World Perf: Episode 676.000000. Reward 49.333333. action: 1.000000. mean reward 59.067337.\n",
      "World Perf: Episode 679.000000. Reward 83.000000. action: 1.000000. mean reward 58.875248.\n",
      "World Perf: Episode 682.000000. Reward 153.666667. action: 0.000000. mean reward 59.359650.\n",
      "World Perf: Episode 685.000000. Reward 106.666667. action: 1.000000. mean reward 62.055378.\n",
      "World Perf: Episode 688.000000. Reward 87.000000. action: 0.000000. mean reward 61.996174.\n",
      "World Perf: Episode 691.000000. Reward 52.333333. action: 0.000000. mean reward 61.743786.\n",
      "World Perf: Episode 694.000000. Reward 32.000000. action: 0.000000. mean reward 60.966694.\n",
      "World Perf: Episode 697.000000. Reward 88.000000. action: 0.000000. mean reward 60.693665.\n",
      "World Perf: Episode 700.000000. Reward 135.666667. action: 0.000000. mean reward 60.917355.\n",
      "World Perf: Episode 703.000000. Reward 72.333333. action: 1.000000. mean reward 62.035030.\n",
      "World Perf: Episode 706.000000. Reward 49.000000. action: 0.000000. mean reward 61.669621.\n",
      "World Perf: Episode 709.000000. Reward 67.666667. action: 0.000000. mean reward 61.285507.\n",
      "World Perf: Episode 712.000000. Reward 78.666667. action: 1.000000. mean reward 60.996883.\n",
      "World Perf: Episode 715.000000. Reward 78.666667. action: 0.000000. mean reward 60.757843.\n",
      "World Perf: Episode 718.000000. Reward 106.666667. action: 0.000000. mean reward 62.029629.\n",
      "World Perf: Episode 721.000000. Reward 107.333333. action: 0.000000. mean reward 61.995407.\n",
      "World Perf: Episode 724.000000. Reward 103.333333. action: 0.000000. mean reward 62.609241.\n",
      "World Perf: Episode 727.000000. Reward 97.666667. action: 0.000000. mean reward 62.445126.\n",
      "World Perf: Episode 730.000000. Reward 60.333333. action: 1.000000. mean reward 61.857746.\n",
      "World Perf: Episode 733.000000. Reward 51.333333. action: 0.000000. mean reward 61.406967.\n",
      "World Perf: Episode 736.000000. Reward 100.333333. action: 0.000000. mean reward 61.291447.\n",
      "World Perf: Episode 739.000000. Reward 126.666667. action: 0.000000. mean reward 62.405720.\n",
      "World Perf: Episode 742.000000. Reward 57.000000. action: 0.000000. mean reward 62.081776.\n",
      "World Perf: Episode 745.000000. Reward 98.000000. action: 1.000000. mean reward 62.225201.\n",
      "World Perf: Episode 748.000000. Reward 131.333333. action: 1.000000. mean reward 65.747665.\n",
      "World Perf: Episode 751.000000. Reward 59.666667. action: 1.000000. mean reward 65.228073.\n",
      "World Perf: Episode 754.000000. Reward 58.666667. action: 0.000000. mean reward 67.042786.\n",
      "World Perf: Episode 757.000000. Reward 79.333333. action: 1.000000. mean reward 66.776344.\n",
      "World Perf: Episode 760.000000. Reward 67.666667. action: 1.000000. mean reward 68.281265.\n",
      "World Perf: Episode 763.000000. Reward 42.000000. action: 0.000000. mean reward 71.408859.\n",
      "World Perf: Episode 766.000000. Reward 53.333333. action: 0.000000. mean reward 70.703568.\n",
      "World Perf: Episode 769.000000. Reward 54.666667. action: 0.000000. mean reward 71.549179.\n",
      "World Perf: Episode 772.000000. Reward 78.666667. action: 0.000000. mean reward 71.217918.\n",
      "World Perf: Episode 775.000000. Reward 95.333333. action: 0.000000. mean reward 71.637444.\n",
      "World Perf: Episode 778.000000. Reward 116.000000. action: 0.000000. mean reward 72.063416.\n",
      "World Perf: Episode 781.000000. Reward 84.000000. action: 0.000000. mean reward 71.593681.\n",
      "World Perf: Episode 784.000000. Reward 114.666667. action: 0.000000. mean reward 71.631561.\n",
      "World Perf: Episode 787.000000. Reward 98.666667. action: 0.000000. mean reward 71.370415.\n",
      "World Perf: Episode 790.000000. Reward 61.000000. action: 0.000000. mean reward 70.822067.\n",
      "World Perf: Episode 793.000000. Reward 61.000000. action: 1.000000. mean reward 70.113930.\n",
      "World Perf: Episode 796.000000. Reward 73.000000. action: 0.000000. mean reward 70.449516.\n",
      "World Perf: Episode 799.000000. Reward 42.333333. action: 0.000000. mean reward 71.914131.\n",
      "World Perf: Episode 802.000000. Reward 57.333333. action: 0.000000. mean reward 71.383217.\n",
      "World Perf: Episode 805.000000. Reward 69.000000. action: 0.000000. mean reward 70.852760.\n",
      "World Perf: Episode 808.000000. Reward 51.333333. action: 0.000000. mean reward 72.077995.\n",
      "World Perf: Episode 811.000000. Reward 124.000000. action: 0.000000. mean reward 74.382034.\n",
      "World Perf: Episode 814.000000. Reward 48.000000. action: 1.000000. mean reward 75.393951.\n",
      "World Perf: Episode 817.000000. Reward 41.333333. action: 0.000000. mean reward 75.371407.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 820.000000. Reward 103.333333. action: 0.000000. mean reward 75.424019.\n",
      "World Perf: Episode 823.000000. Reward 36.333333. action: 0.000000. mean reward 77.336006.\n",
      "World Perf: Episode 826.000000. Reward 84.333333. action: 0.000000. mean reward 80.044983.\n",
      "World Perf: Episode 829.000000. Reward 56.333333. action: 0.000000. mean reward 79.350853.\n",
      "World Perf: Episode 832.000000. Reward 79.666667. action: 1.000000. mean reward 78.764214.\n",
      "World Perf: Episode 835.000000. Reward 77.333333. action: 0.000000. mean reward 81.713387.\n",
      "World Perf: Episode 838.000000. Reward 58.333333. action: 0.000000. mean reward 80.911804.\n",
      "World Perf: Episode 841.000000. Reward 91.666667. action: 0.000000. mean reward 80.378761.\n",
      "World Perf: Episode 844.000000. Reward 58.000000. action: 0.000000. mean reward 81.349953.\n",
      "World Perf: Episode 847.000000. Reward 116.666667. action: 0.000000. mean reward 81.286018.\n",
      "World Perf: Episode 850.000000. Reward 51.000000. action: 0.000000. mean reward 80.377235.\n",
      "World Perf: Episode 853.000000. Reward 109.333333. action: 0.000000. mean reward 80.616005.\n",
      "World Perf: Episode 856.000000. Reward 83.666667. action: 0.000000. mean reward 82.202240.\n",
      "World Perf: Episode 859.000000. Reward 30.333333. action: 0.000000. mean reward 84.321609.\n",
      "World Perf: Episode 862.000000. Reward 106.666667. action: 0.000000. mean reward 84.506645.\n",
      "World Perf: Episode 865.000000. Reward 24.666667. action: 0.000000. mean reward 86.144814.\n",
      "World Perf: Episode 868.000000. Reward 53.333333. action: 0.000000. mean reward 86.668419.\n",
      "World Perf: Episode 871.000000. Reward 86.000000. action: 0.000000. mean reward 86.131859.\n",
      "World Perf: Episode 874.000000. Reward 47.666667. action: 1.000000. mean reward 85.657677.\n",
      "World Perf: Episode 877.000000. Reward 61.333333. action: 0.000000. mean reward 87.515633.\n",
      "World Perf: Episode 880.000000. Reward 114.333333. action: 0.000000. mean reward 89.916267.\n",
      "World Perf: Episode 883.000000. Reward 72.000000. action: 0.000000. mean reward 88.999535.\n",
      "World Perf: Episode 886.000000. Reward 104.333333. action: 0.000000. mean reward 88.381584.\n",
      "World Perf: Episode 889.000000. Reward 146.333333. action: 0.000000. mean reward 94.604851.\n",
      "World Perf: Episode 892.000000. Reward 89.000000. action: 0.000000. mean reward 103.362617.\n",
      "World Perf: Episode 895.000000. Reward 110.666667. action: 0.000000. mean reward 102.527290.\n",
      "World Perf: Episode 898.000000. Reward 80.666667. action: 0.000000. mean reward 101.503899.\n",
      "World Perf: Episode 901.000000. Reward 88.000000. action: 0.000000. mean reward 100.619301.\n",
      "World Perf: Episode 904.000000. Reward 70.333333. action: 0.000000. mean reward 102.458382.\n",
      "World Perf: Episode 907.000000. Reward 29.000000. action: 0.000000. mean reward 101.051903.\n",
      "World Perf: Episode 910.000000. Reward 165.666667. action: 0.000000. mean reward 104.806175.\n",
      "World Perf: Episode 913.000000. Reward 65.333333. action: 1.000000. mean reward 106.313263.\n",
      "World Perf: Episode 916.000000. Reward 116.000000. action: 0.000000. mean reward 108.436852.\n",
      "World Perf: Episode 919.000000. Reward 153.333333. action: 0.000000. mean reward 110.777351.\n",
      "World Perf: Episode 922.000000. Reward 53.666667. action: 1.000000. mean reward 112.008369.\n",
      "World Perf: Episode 925.000000. Reward 82.000000. action: 1.000000. mean reward 113.782349.\n",
      "World Perf: Episode 928.000000. Reward 125.666667. action: 1.000000. mean reward 113.119698.\n",
      "World Perf: Episode 931.000000. Reward 170.666667. action: 0.000000. mean reward 113.714500.\n",
      "World Perf: Episode 934.000000. Reward 44.000000. action: 0.000000. mean reward 111.963226.\n",
      "World Perf: Episode 937.000000. Reward 36.666667. action: 0.000000. mean reward 114.397911.\n",
      "World Perf: Episode 940.000000. Reward 103.000000. action: 1.000000. mean reward 134.042953.\n",
      "World Perf: Episode 943.000000. Reward 135.000000. action: 0.000000. mean reward 133.141296.\n",
      "World Perf: Episode 946.000000. Reward 129.333333. action: 0.000000. mean reward 134.771042.\n",
      "World Perf: Episode 949.000000. Reward 161.000000. action: 1.000000. mean reward 136.849182.\n",
      "World Perf: Episode 952.000000. Reward 134.333333. action: 0.000000. mean reward 138.891678.\n",
      "World Perf: Episode 955.000000. Reward 141.000000. action: 0.000000. mean reward 141.052490.\n",
      "World Perf: Episode 958.000000. Reward 143.000000. action: 0.000000. mean reward 140.119858.\n",
      "World Perf: Episode 961.000000. Reward 92.666667. action: 0.000000. mean reward 138.697449.\n",
      "World Perf: Episode 964.000000. Reward 95.333333. action: 0.000000. mean reward 138.797806.\n",
      "World Perf: Episode 967.000000. Reward 81.000000. action: 0.000000. mean reward 138.939804.\n",
      "World Perf: Episode 970.000000. Reward 68.000000. action: 1.000000. mean reward 138.481018.\n",
      "World Perf: Episode 973.000000. Reward 73.666667. action: 1.000000. mean reward 138.648834.\n",
      "World Perf: Episode 976.000000. Reward 70.333333. action: 0.000000. mean reward 136.967911.\n",
      "World Perf: Episode 979.000000. Reward 127.666667. action: 0.000000. mean reward 138.533371.\n",
      "World Perf: Episode 982.000000. Reward 102.333333. action: 1.000000. mean reward 139.722153.\n",
      "World Perf: Episode 985.000000. Reward 166.000000. action: 0.000000. mean reward 142.189453.\n",
      "World Perf: Episode 988.000000. Reward 116.666667. action: 0.000000. mean reward 140.848312.\n",
      "World Perf: Episode 991.000000. Reward 147.333333. action: 0.000000. mean reward 140.160568.\n",
      "World Perf: Episode 994.000000. Reward 84.000000. action: 0.000000. mean reward 141.201431.\n",
      "World Perf: Episode 997.000000. Reward 141.666667. action: 1.000000. mean reward 141.122681.\n",
      "World Perf: Episode 1000.000000. Reward 67.333333. action: 1.000000. mean reward 140.492844.\n",
      "World Perf: Episode 1003.000000. Reward 76.000000. action: 0.000000. mean reward 139.971039.\n",
      "World Perf: Episode 1006.000000. Reward 75.666667. action: 1.000000. mean reward 141.209839.\n",
      "World Perf: Episode 1009.000000. Reward 197.000000. action: 1.000000. mean reward 143.304916.\n",
      "World Perf: Episode 1012.000000. Reward 88.000000. action: 1.000000. mean reward 144.230667.\n",
      "World Perf: Episode 1015.000000. Reward 134.000000. action: 1.000000. mean reward 142.861694.\n",
      "World Perf: Episode 1018.000000. Reward 68.333333. action: 0.000000. mean reward 141.138947.\n",
      "World Perf: Episode 1021.000000. Reward 92.666667. action: 0.000000. mean reward 139.388351.\n",
      "World Perf: Episode 1024.000000. Reward 52.333333. action: 0.000000. mean reward 137.390030.\n",
      "World Perf: Episode 1027.000000. Reward 66.666667. action: 1.000000. mean reward 135.498459.\n",
      "World Perf: Episode 1030.000000. Reward 135.666667. action: 1.000000. mean reward 134.421188.\n",
      "World Perf: Episode 1033.000000. Reward 160.333333. action: 1.000000. mean reward 133.502350.\n",
      "World Perf: Episode 1036.000000. Reward 139.666667. action: 0.000000. mean reward 132.579330.\n",
      "World Perf: Episode 1039.000000. Reward 118.000000. action: 0.000000. mean reward 134.133591.\n",
      "World Perf: Episode 1042.000000. Reward 122.333333. action: 0.000000. mean reward 133.615738.\n",
      "World Perf: Episode 1045.000000. Reward 83.666667. action: 0.000000. mean reward 133.114227.\n",
      "World Perf: Episode 1048.000000. Reward 129.666667. action: 0.000000. mean reward 132.325272.\n",
      "World Perf: Episode 1051.000000. Reward 106.000000. action: 1.000000. mean reward 130.913651.\n",
      "World Perf: Episode 1054.000000. Reward 58.666667. action: 1.000000. mean reward 129.446152.\n",
      "World Perf: Episode 1057.000000. Reward 153.333333. action: 0.000000. mean reward 129.070831.\n",
      "World Perf: Episode 1060.000000. Reward 154.333333. action: 1.000000. mean reward 128.599197.\n",
      "World Perf: Episode 1063.000000. Reward 138.666667. action: 0.000000. mean reward 127.733368.\n",
      "World Perf: Episode 1066.000000. Reward 119.000000. action: 0.000000. mean reward 129.130478.\n",
      "World Perf: Episode 1069.000000. Reward 84.666667. action: 1.000000. mean reward 129.639618.\n",
      "World Perf: Episode 1072.000000. Reward 62.666667. action: 0.000000. mean reward 129.611633.\n",
      "World Perf: Episode 1075.000000. Reward 188.333333. action: 1.000000. mean reward 131.044235.\n",
      "World Perf: Episode 1078.000000. Reward 74.000000. action: 1.000000. mean reward 132.278275.\n",
      "World Perf: Episode 1081.000000. Reward 102.666667. action: 0.000000. mean reward 130.829880.\n",
      "World Perf: Episode 1084.000000. Reward 130.333333. action: 0.000000. mean reward 130.090195.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1087.000000. Reward 152.000000. action: 0.000000. mean reward 129.280716.\n",
      "World Perf: Episode 1090.000000. Reward 155.000000. action: 0.000000. mean reward 128.482651.\n",
      "World Perf: Episode 1093.000000. Reward 158.000000. action: 0.000000. mean reward 127.743248.\n",
      "World Perf: Episode 1096.000000. Reward 63.333333. action: 0.000000. mean reward 125.944366.\n",
      "World Perf: Episode 1099.000000. Reward 151.666667. action: 1.000000. mean reward 126.999542.\n",
      "World Perf: Episode 1102.000000. Reward 118.333333. action: 0.000000. mean reward 126.575714.\n",
      "World Perf: Episode 1105.000000. Reward 91.333333. action: 1.000000. mean reward 125.214539.\n",
      "World Perf: Episode 1108.000000. Reward 142.333333. action: 1.000000. mean reward 124.255699.\n",
      "World Perf: Episode 1111.000000. Reward 170.000000. action: 0.000000. mean reward 124.230011.\n",
      "World Perf: Episode 1114.000000. Reward 72.000000. action: 0.000000. mean reward 122.755379.\n",
      "World Perf: Episode 1117.000000. Reward 160.333333. action: 0.000000. mean reward 122.074982.\n",
      "World Perf: Episode 1120.000000. Reward 120.000000. action: 0.000000. mean reward 123.071899.\n",
      "World Perf: Episode 1123.000000. Reward 71.666667. action: 0.000000. mean reward 123.551399.\n",
      "World Perf: Episode 1126.000000. Reward 177.666667. action: 1.000000. mean reward 123.157494.\n",
      "World Perf: Episode 1129.000000. Reward 141.666667. action: 1.000000. mean reward 122.513496.\n",
      "World Perf: Episode 1132.000000. Reward 163.333333. action: 1.000000. mean reward 124.056160.\n",
      "World Perf: Episode 1135.000000. Reward 146.333333. action: 1.000000. mean reward 123.493492.\n",
      "World Perf: Episode 1138.000000. Reward 95.000000. action: 0.000000. mean reward 122.392860.\n",
      "World Perf: Episode 1141.000000. Reward 150.000000. action: 1.000000. mean reward 121.580261.\n",
      "World Perf: Episode 1144.000000. Reward 92.000000. action: 1.000000. mean reward 120.434692.\n",
      "World Perf: Episode 1147.000000. Reward 200.000000. action: 0.000000. mean reward 121.711334.\n",
      "World Perf: Episode 1150.000000. Reward 182.333333. action: 0.000000. mean reward 122.582680.\n",
      "World Perf: Episode 1153.000000. Reward 130.333333. action: 0.000000. mean reward 122.473846.\n",
      "World Perf: Episode 1156.000000. Reward 129.333333. action: 0.000000. mean reward 121.923653.\n",
      "World Perf: Episode 1159.000000. Reward 172.333333. action: 0.000000. mean reward 121.599632.\n",
      "World Perf: Episode 1162.000000. Reward 171.666667. action: 0.000000. mean reward 121.197258.\n",
      "World Perf: Episode 1165.000000. Reward 182.666667. action: 0.000000. mean reward 121.977936.\n",
      "World Perf: Episode 1168.000000. Reward 176.000000. action: 0.000000. mean reward 123.307320.\n",
      "World Perf: Episode 1171.000000. Reward 191.333333. action: 1.000000. mean reward 124.026093.\n",
      "World Perf: Episode 1174.000000. Reward 200.000000. action: 0.000000. mean reward 126.486755.\n",
      "World Perf: Episode 1177.000000. Reward 143.666667. action: 1.000000. mean reward 125.707764.\n",
      "World Perf: Episode 1180.000000. Reward 200.000000. action: 0.000000. mean reward 128.247879.\n",
      "World Perf: Episode 1183.000000. Reward 198.666667. action: 0.000000. mean reward 130.887589.\n",
      "World Perf: Episode 1186.000000. Reward 98.000000. action: 0.000000. mean reward 130.081467.\n",
      "World Perf: Episode 1189.000000. Reward 120.666667. action: 1.000000. mean reward 131.216141.\n",
      "World Perf: Episode 1192.000000. Reward 147.666667. action: 1.000000. mean reward 131.628510.\n",
      "World Perf: Episode 1195.000000. Reward 153.000000. action: 1.000000. mean reward 132.963531.\n",
      "World Perf: Episode 1198.000000. Reward 101.333333. action: 0.000000. mean reward 131.629135.\n",
      "World Perf: Episode 1201.000000. Reward 151.333333. action: 0.000000. mean reward 130.852249.\n",
      "World Perf: Episode 1204.000000. Reward 154.666667. action: 0.000000. mean reward 129.965912.\n",
      "World Perf: Episode 1207.000000. Reward 155.666667. action: 1.000000. mean reward 129.062576.\n",
      "World Perf: Episode 1210.000000. Reward 152.333333. action: 0.000000. mean reward 130.178116.\n",
      "World Perf: Episode 1213.000000. Reward 200.000000. action: 0.000000. mean reward 131.908051.\n",
      "World Perf: Episode 1216.000000. Reward 195.333333. action: 1.000000. mean reward 132.411194.\n",
      "World Perf: Episode 1219.000000. Reward 200.000000. action: 0.000000. mean reward 135.060989.\n",
      "World Perf: Episode 1222.000000. Reward 130.000000. action: 1.000000. mean reward 136.809601.\n",
      "World Perf: Episode 1225.000000. Reward 128.333333. action: 1.000000. mean reward 135.483826.\n",
      "World Perf: Episode 1228.000000. Reward 199.000000. action: 0.000000. mean reward 136.806351.\n",
      "World Perf: Episode 1231.000000. Reward 183.333333. action: 0.000000. mean reward 138.878433.\n",
      "World Perf: Episode 1234.000000. Reward 200.000000. action: 1.000000. mean reward 138.230270.\n",
      "World Perf: Episode 1237.000000. Reward 143.333333. action: 0.000000. mean reward 137.334946.\n",
      "World Perf: Episode 1240.000000. Reward 200.000000. action: 0.000000. mean reward 139.543182.\n",
      "World Perf: Episode 1243.000000. Reward 178.666667. action: 0.000000. mean reward 141.591888.\n",
      "World Perf: Episode 1246.000000. Reward 199.000000. action: 0.000000. mean reward 143.871933.\n",
      "World Perf: Episode 1249.000000. Reward 194.000000. action: 0.000000. mean reward 143.038986.\n",
      "World Perf: Episode 1252.000000. Reward 148.000000. action: 0.000000. mean reward 142.202164.\n",
      "World Perf: Episode 1255.000000. Reward 173.666667. action: 1.000000. mean reward 143.124619.\n",
      "World Perf: Episode 1258.000000. Reward 185.666667. action: 1.000000. mean reward 145.048386.\n",
      "World Perf: Episode 1261.000000. Reward 189.666667. action: 0.000000. mean reward 147.071121.\n",
      "World Perf: Episode 1264.000000. Reward 142.000000. action: 0.000000. mean reward 147.415771.\n",
      "World Perf: Episode 1267.000000. Reward 197.333333. action: 1.000000. mean reward 146.859039.\n",
      "World Perf: Episode 1270.000000. Reward 194.333333. action: 0.000000. mean reward 148.833542.\n",
      "World Perf: Episode 1273.000000. Reward 182.333333. action: 1.000000. mean reward 148.899475.\n",
      "World Perf: Episode 1276.000000. Reward 166.333333. action: 1.000000. mean reward 147.725967.\n",
      "World Perf: Episode 1279.000000. Reward 200.000000. action: 1.000000. mean reward 149.843994.\n",
      "World Perf: Episode 1282.000000. Reward 154.333333. action: 1.000000. mean reward 150.150650.\n",
      "World Perf: Episode 1285.000000. Reward 114.000000. action: 0.000000. mean reward 148.498413.\n",
      "World Perf: Episode 1288.000000. Reward 176.000000. action: 0.000000. mean reward 150.431717.\n",
      "World Perf: Episode 1291.000000. Reward 154.666667. action: 1.000000. mean reward 151.989639.\n",
      "World Perf: Episode 1294.000000. Reward 199.333333. action: 1.000000. mean reward 151.542496.\n",
      "World Perf: Episode 1297.000000. Reward 174.333333. action: 0.000000. mean reward 150.380600.\n",
      "World Perf: Episode 1300.000000. Reward 200.000000. action: 0.000000. mean reward 151.078018.\n",
      "World Perf: Episode 1303.000000. Reward 200.000000. action: 0.000000. mean reward 152.827805.\n",
      "World Perf: Episode 1306.000000. Reward 192.333333. action: 0.000000. mean reward 153.787491.\n",
      "World Perf: Episode 1309.000000. Reward 200.000000. action: 0.000000. mean reward 153.997650.\n",
      "World Perf: Episode 1312.000000. Reward 152.333333. action: 0.000000. mean reward 153.018463.\n",
      "World Perf: Episode 1315.000000. Reward 200.000000. action: 1.000000. mean reward 154.218399.\n",
      "World Perf: Episode 1318.000000. Reward 173.000000. action: 0.000000. mean reward 156.247025.\n",
      "World Perf: Episode 1321.000000. Reward 160.666667. action: 0.000000. mean reward 155.298309.\n",
      "World Perf: Episode 1324.000000. Reward 198.000000. action: 1.000000. mean reward 154.756805.\n",
      "World Perf: Episode 1327.000000. Reward 183.666667. action: 1.000000. mean reward 155.032867.\n",
      "World Perf: Episode 1330.000000. Reward 200.000000. action: 1.000000. mean reward 156.005264.\n",
      "World Perf: Episode 1333.000000. Reward 195.666667. action: 1.000000. mean reward 157.827591.\n",
      "World Perf: Episode 1336.000000. Reward 190.333333. action: 1.000000. mean reward 159.503693.\n",
      "World Perf: Episode 1339.000000. Reward 156.333333. action: 0.000000. mean reward 158.912628.\n",
      "World Perf: Episode 1342.000000. Reward 200.000000. action: 1.000000. mean reward 159.904510.\n",
      "World Perf: Episode 1345.000000. Reward 200.000000. action: 1.000000. mean reward 160.688232.\n",
      "World Perf: Episode 1348.000000. Reward 200.000000. action: 1.000000. mean reward 162.566513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1351.000000. Reward 121.333333. action: 0.000000. mean reward 161.156174.\n",
      "World Perf: Episode 1354.000000. Reward 200.000000. action: 0.000000. mean reward 160.188278.\n",
      "World Perf: Episode 1357.000000. Reward 175.333333. action: 1.000000. mean reward 160.021240.\n",
      "World Perf: Episode 1360.000000. Reward 194.666667. action: 1.000000. mean reward 161.758118.\n",
      "World Perf: Episode 1363.000000. Reward 200.000000. action: 1.000000. mean reward 161.673492.\n",
      "World Perf: Episode 1366.000000. Reward 196.000000. action: 0.000000. mean reward 161.571671.\n",
      "World Perf: Episode 1369.000000. Reward 154.666667. action: 0.000000. mean reward 161.998718.\n",
      "World Perf: Episode 1372.000000. Reward 170.000000. action: 0.000000. mean reward 163.783325.\n",
      "World Perf: Episode 1375.000000. Reward 187.666667. action: 0.000000. mean reward 164.505859.\n",
      "World Perf: Episode 1378.000000. Reward 200.000000. action: 0.000000. mean reward 164.343155.\n",
      "World Perf: Episode 1381.000000. Reward 153.333333. action: 0.000000. mean reward 165.638443.\n",
      "World Perf: Episode 1384.000000. Reward 200.000000. action: 1.000000. mean reward 165.658966.\n",
      "World Perf: Episode 1387.000000. Reward 182.000000. action: 0.000000. mean reward 164.783478.\n",
      "World Perf: Episode 1390.000000. Reward 181.666667. action: 0.000000. mean reward 163.634018.\n",
      "World Perf: Episode 1393.000000. Reward 134.000000. action: 0.000000. mean reward 163.745407.\n",
      "World Perf: Episode 1396.000000. Reward 165.666667. action: 0.000000. mean reward 165.069794.\n",
      "World Perf: Episode 1399.000000. Reward 183.000000. action: 0.000000. mean reward 163.845261.\n",
      "World Perf: Episode 1402.000000. Reward 187.666667. action: 0.000000. mean reward 165.016922.\n",
      "World Perf: Episode 1405.000000. Reward 135.000000. action: 1.000000. mean reward 166.019409.\n",
      "World Perf: Episode 1408.000000. Reward 183.000000. action: 0.000000. mean reward 164.958755.\n",
      "World Perf: Episode 1411.000000. Reward 192.333333. action: 0.000000. mean reward 163.848862.\n",
      "World Perf: Episode 1414.000000. Reward 200.000000. action: 0.000000. mean reward 163.135300.\n",
      "World Perf: Episode 1417.000000. Reward 191.000000. action: 1.000000. mean reward 162.064590.\n",
      "World Perf: Episode 1420.000000. Reward 182.000000. action: 0.000000. mean reward 163.853760.\n",
      "World Perf: Episode 1423.000000. Reward 186.333333. action: 0.000000. mean reward 162.734070.\n",
      "World Perf: Episode 1426.000000. Reward 200.000000. action: 1.000000. mean reward 164.522278.\n",
      "World Perf: Episode 1429.000000. Reward 194.333333. action: 0.000000. mean reward 166.151230.\n",
      "World Perf: Episode 1432.000000. Reward 193.333333. action: 1.000000. mean reward 167.752396.\n",
      "World Perf: Episode 1435.000000. Reward 180.666667. action: 1.000000. mean reward 167.421478.\n",
      "World Perf: Episode 1438.000000. Reward 95.666667. action: 0.000000. mean reward 168.158081.\n",
      "World Perf: Episode 1441.000000. Reward 196.333333. action: 1.000000. mean reward 168.501450.\n",
      "World Perf: Episode 1444.000000. Reward 200.000000. action: 1.000000. mean reward 167.387924.\n",
      "World Perf: Episode 1447.000000. Reward 185.000000. action: 0.000000. mean reward 168.897217.\n",
      "World Perf: Episode 1450.000000. Reward 174.333333. action: 1.000000. mean reward 167.547424.\n",
      "World Perf: Episode 1453.000000. Reward 191.000000. action: 1.000000. mean reward 169.038223.\n",
      "World Perf: Episode 1456.000000. Reward 197.000000. action: 0.000000. mean reward 168.890579.\n",
      "World Perf: Episode 1459.000000. Reward 200.000000. action: 1.000000. mean reward 167.906601.\n",
      "World Perf: Episode 1462.000000. Reward 145.333333. action: 0.000000. mean reward 167.032883.\n",
      "World Perf: Episode 1465.000000. Reward 200.000000. action: 1.000000. mean reward 165.874161.\n",
      "World Perf: Episode 1468.000000. Reward 193.333333. action: 0.000000. mean reward 167.302734.\n",
      "World Perf: Episode 1471.000000. Reward 168.333333. action: 0.000000. mean reward 168.446060.\n",
      "World Perf: Episode 1474.000000. Reward 200.000000. action: 0.000000. mean reward 168.141098.\n",
      "World Perf: Episode 1477.000000. Reward 200.000000. action: 1.000000. mean reward 169.734604.\n",
      "World Perf: Episode 1480.000000. Reward 200.000000. action: 1.000000. mean reward 170.590744.\n",
      "World Perf: Episode 1483.000000. Reward 200.000000. action: 0.000000. mean reward 169.419724.\n",
      "World Perf: Episode 1486.000000. Reward 200.000000. action: 1.000000. mean reward 168.188858.\n",
      "World Perf: Episode 1489.000000. Reward 200.000000. action: 0.000000. mean reward 167.070633.\n",
      "World Perf: Episode 1492.000000. Reward 168.666667. action: 0.000000. mean reward 167.782608.\n",
      "World Perf: Episode 1495.000000. Reward 200.000000. action: 0.000000. mean reward 169.219925.\n",
      "World Perf: Episode 1498.000000. Reward 179.000000. action: 0.000000. mean reward 169.868027.\n",
      "World Perf: Episode 1501.000000. Reward 200.000000. action: 0.000000. mean reward 171.408615.\n",
      "World Perf: Episode 1504.000000. Reward 122.666667. action: 0.000000. mean reward 169.963776.\n",
      "World Perf: Episode 1507.000000. Reward 197.666667. action: 0.000000. mean reward 171.502869.\n",
      "World Perf: Episode 1510.000000. Reward 183.333333. action: 0.000000. mean reward 170.696899.\n",
      "World Perf: Episode 1513.000000. Reward 200.000000. action: 0.000000. mean reward 172.303726.\n",
      "World Perf: Episode 1516.000000. Reward 199.666667. action: 0.000000. mean reward 173.850876.\n",
      "World Perf: Episode 1519.000000. Reward 150.000000. action: 0.000000. mean reward 173.085449.\n",
      "World Perf: Episode 1522.000000. Reward 200.000000. action: 1.000000. mean reward 172.615158.\n",
      "World Perf: Episode 1525.000000. Reward 189.666667. action: 0.000000. mean reward 171.568283.\n",
      "World Perf: Episode 1528.000000. Reward 200.000000. action: 0.000000. mean reward 170.534744.\n",
      "World Perf: Episode 1531.000000. Reward 187.333333. action: 0.000000. mean reward 169.464478.\n",
      "World Perf: Episode 1534.000000. Reward 200.000000. action: 0.000000. mean reward 168.439941.\n",
      "World Perf: Episode 1537.000000. Reward 200.000000. action: 0.000000. mean reward 167.964249.\n",
      "World Perf: Episode 1540.000000. Reward 189.333333. action: 0.000000. mean reward 166.668533.\n",
      "World Perf: Episode 1543.000000. Reward 200.000000. action: 0.000000. mean reward 165.462997.\n",
      "World Perf: Episode 1546.000000. Reward 200.000000. action: 1.000000. mean reward 166.092300.\n",
      "World Perf: Episode 1549.000000. Reward 193.333333. action: 0.000000. mean reward 167.951096.\n",
      "World Perf: Episode 1552.000000. Reward 200.000000. action: 0.000000. mean reward 166.746414.\n",
      "World Perf: Episode 1555.000000. Reward 200.000000. action: 0.000000. mean reward 166.955704.\n",
      "World Perf: Episode 1558.000000. Reward 137.666667. action: 1.000000. mean reward 168.104324.\n",
      "World Perf: Episode 1561.000000. Reward 200.000000. action: 1.000000. mean reward 168.617752.\n",
      "World Perf: Episode 1564.000000. Reward 200.000000. action: 1.000000. mean reward 169.568802.\n",
      "World Perf: Episode 1567.000000. Reward 173.000000. action: 0.000000. mean reward 170.874710.\n",
      "World Perf: Episode 1570.000000. Reward 197.333333. action: 1.000000. mean reward 172.471298.\n",
      "World Perf: Episode 1573.000000. Reward 190.666667. action: 1.000000. mean reward 174.013535.\n",
      "World Perf: Episode 1576.000000. Reward 199.000000. action: 1.000000. mean reward 175.506393.\n",
      "World Perf: Episode 1579.000000. Reward 200.000000. action: 1.000000. mean reward 176.986130.\n",
      "World Perf: Episode 1582.000000. Reward 200.000000. action: 0.000000. mean reward 178.441650.\n",
      "World Perf: Episode 1585.000000. Reward 200.000000. action: 0.000000. mean reward 179.808578.\n",
      "World Perf: Episode 1588.000000. Reward 200.000000. action: 1.000000. mean reward 180.224487.\n",
      "World Perf: Episode 1591.000000. Reward 200.000000. action: 0.000000. mean reward 181.717102.\n",
      "World Perf: Episode 1594.000000. Reward 196.666667. action: 0.000000. mean reward 182.111328.\n",
      "World Perf: Episode 1597.000000. Reward 139.666667. action: 0.000000. mean reward 182.672668.\n",
      "World Perf: Episode 1600.000000. Reward 200.000000. action: 0.000000. mean reward 183.342789.\n",
      "World Perf: Episode 1603.000000. Reward 200.000000. action: 1.000000. mean reward 184.497787.\n",
      "World Perf: Episode 1606.000000. Reward 200.000000. action: 1.000000. mean reward 183.223587.\n",
      "World Perf: Episode 1609.000000. Reward 200.000000. action: 0.000000. mean reward 183.064987.\n",
      "World Perf: Episode 1612.000000. Reward 200.000000. action: 1.000000. mean reward 182.979980.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1615.000000. Reward 200.000000. action: 1.000000. mean reward 182.175217.\n",
      "World Perf: Episode 1618.000000. Reward 196.666667. action: 0.000000. mean reward 181.220047.\n",
      "World Perf: Episode 1621.000000. Reward 188.666667. action: 1.000000. mean reward 182.555984.\n",
      "World Perf: Episode 1624.000000. Reward 198.333333. action: 0.000000. mean reward 183.134140.\n",
      "World Perf: Episode 1627.000000. Reward 200.000000. action: 0.000000. mean reward 181.700012.\n",
      "World Perf: Episode 1630.000000. Reward 200.000000. action: 1.000000. mean reward 181.302429.\n",
      "World Perf: Episode 1633.000000. Reward 200.000000. action: 0.000000. mean reward 182.752670.\n",
      "World Perf: Episode 1636.000000. Reward 200.000000. action: 1.000000. mean reward 184.260330.\n",
      "World Perf: Episode 1639.000000. Reward 200.000000. action: 1.000000. mean reward 185.619034.\n",
      "World Perf: Episode 1642.000000. Reward 200.000000. action: 0.000000. mean reward 186.225876.\n",
      "World Perf: Episode 1645.000000. Reward 200.000000. action: 1.000000. mean reward 187.290939.\n",
      "World Perf: Episode 1648.000000. Reward 200.000000. action: 1.000000. mean reward 188.774033.\n",
      "World Perf: Episode 1651.000000. Reward 188.333333. action: 0.000000. mean reward 189.749329.\n",
      "World Perf: Episode 1654.000000. Reward 200.000000. action: 0.000000. mean reward 190.492752.\n",
      "World Perf: Episode 1657.000000. Reward 200.000000. action: 0.000000. mean reward 191.750748.\n",
      "World Perf: Episode 1660.000000. Reward 186.333333. action: 0.000000. mean reward 192.725845.\n",
      "World Perf: Episode 1663.000000. Reward 197.666667. action: 1.000000. mean reward 192.858292.\n",
      "World Perf: Episode 1666.000000. Reward 157.000000. action: 0.000000. mean reward 193.580856.\n",
      "World Perf: Episode 1669.000000. Reward 200.000000. action: 1.000000. mean reward 194.685196.\n",
      "World Perf: Episode 1672.000000. Reward 200.000000. action: 0.000000. mean reward 195.770142.\n",
      "World Perf: Episode 1675.000000. Reward 200.000000. action: 0.000000. mean reward 196.906982.\n",
      "World Perf: Episode 1678.000000. Reward 188.000000. action: 0.000000. mean reward 197.786011.\n",
      "World Perf: Episode 1681.000000. Reward 200.000000. action: 0.000000. mean reward 198.835312.\n",
      "World Perf: Episode 1684.000000. Reward 200.000000. action: 0.000000. mean reward 199.850052.\n",
      "World Perf: Episode 1687.000000. Reward 200.000000. action: 0.000000. mean reward 199.180252.\n",
      "World Perf: Episode 1690.000000. Reward 200.000000. action: 1.000000. mean reward 200.204636.\n",
      "World Perf: Episode 1693.000000. Reward 144.666667. action: 1.000000. mean reward 200.701431.\n",
      "World Perf: Episode 1696.000000. Reward 200.000000. action: 0.000000. mean reward 199.835144.\n",
      "World Perf: Episode 1699.000000. Reward 200.000000. action: 0.000000. mean reward 198.721146.\n",
      "World Perf: Episode 1702.000000. Reward 194.000000. action: 0.000000. mean reward 199.665298.\n",
      "World Perf: Episode 1705.000000. Reward 200.000000. action: 1.000000. mean reward 200.658524.\n",
      "World Perf: Episode 1708.000000. Reward 195.666667. action: 1.000000. mean reward 200.881577.\n",
      "World Perf: Episode 1711.000000. Reward 200.000000. action: 0.000000. mean reward 201.839401.\n",
      "World Perf: Episode 1714.000000. Reward 200.000000. action: 0.000000. mean reward 202.830688.\n",
      "World Perf: Episode 1717.000000. Reward 200.000000. action: 1.000000. mean reward 201.410141.\n",
      "World Perf: Episode 1720.000000. Reward 200.000000. action: 1.000000. mean reward 202.393417.\n",
      "World Perf: Episode 1723.000000. Reward 194.666667. action: 0.000000. mean reward 202.354004.\n",
      "World Perf: Episode 1726.000000. Reward 200.000000. action: 1.000000. mean reward 203.313919.\n",
      "World Perf: Episode 1729.000000. Reward 200.000000. action: 1.000000. mean reward 204.211899.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fca3075cbeea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mepisode_number\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_episodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m150\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdrawFromModel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m_wait_vsync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXGetVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXWaitVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs, drs, ys, ds = [], [], [], []  # reset history\n",
    "total_episodes = 2000\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False    # True if DNN model is used to predict observations\n",
    "trainModel = True        # True if DNN for model must be trained\n",
    "trainPolicy = False      # True if Actor DNN must be trained\n",
    "\n",
    "switch_point = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number < total_episodes:\n",
    "        if (reward_sum/batch_size > 150 and not drawFromModel) or rendering:\n",
    "            env.render()\n",
    "            rendering = True\n",
    "        x = np.reshape(observation, [1, 4])\n",
    "        \n",
    "        # Actor DNN action prediction\n",
    "        tfprob = sess.run(probability, feed_dict={observations: x})\n",
    "        if np.random.uniform() < tfprob:\n",
    "            action = 1\n",
    "            y = 0\n",
    "        else:\n",
    "            action = 0\n",
    "            y = 1\n",
    "            \n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        \n",
    "        # Model/Real environment step\n",
    "        if not drawFromModel:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else: \n",
    "            observation, reward, done = stepModel(sess, xs, action)\n",
    "            \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward)\n",
    "        \n",
    "        # Update DNNs if episode is done\n",
    "        if done:\n",
    "            if not drawFromModel:\n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "            \n",
    "            # Stacking all obs, rewards, done from episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            # reset history buffer\n",
    "            xs, drs, ys, ds = [], [], [], []\n",
    "            \n",
    "            if trainModel:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1, :]\n",
    "                state_prevs = np.hstack([state_prevs, actions])\n",
    "                state_nexts = epx[1:, :]\n",
    "                rewards = np.array(epr[1:, :])\n",
    "                dones = np.array(epd[1:, :])\n",
    "                state_nextsAll = np.hstack([state_nexts, rewards, dones])\n",
    "                \n",
    "                feed_dict = {previous_state: state_prevs, true_observation: state_nexts, \\\n",
    "                             true_done: dones, true_reward: rewards}\n",
    "                loss, pStat, _ = sess.run([model_loss, predicted_state, updateModel], feed_dict=feed_dict)\n",
    "            \n",
    "            if trainPolicy:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads, feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                \n",
    "                # break if gradients are too large\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix, grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "            \n",
    "            # Update Actor weights every switch_point+batch_size episodes \n",
    "            # Actor gradients are computed at each step (and Actor DNN is updated)\n",
    "            if switch_point + batch_size == episode_number:\n",
    "                switch_point = episode_number\n",
    "                if trainPolicy:\n",
    "                    sess.run(updateGrads, feed_dict={W1Grad: gradBuffer[0], W2Grad: gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "                    \n",
    "                if running_reward is None:\n",
    "                    running_reward = reward_sum\n",
    "                else:\n",
    "                    running_reward = running_reward * gamma + reward_sum * (1 - gamma)\n",
    "                \n",
    "                if not drawFromModel:\n",
    "                    print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainModel = not trainModel\n",
    "                    trainPolicy = not trainPolicy\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
